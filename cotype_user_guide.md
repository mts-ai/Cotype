# Перечень терминов

- **Искусственный интеллект** – область компьютерных наук, занимающаяся созданием вычислительных систем, способных выполнять задачи, требующие человеческого интеллекта, такие как восприятие, рассуждение, обучение и решение проблем.
- **Машинное обучение** – раздел искусственного интеллекта, в котором вычислительные системы обучаются выполнять задачи, анализируя и обобщая данные. Обучение происходит без явного программирования специфических инструкций.
- **Генеративные модели** – типы искусственного интеллекта, способные создавать новый контент, включая тексты, изображения и музыку. Такие модели обучаются на больших объемах данных и затем генерируют новый контент, имитируя наблюдаемые данные.
- **Дообучение (Fine-tuning)** – процесс настройки предобученной модели под конкретную задачу путем дополнительного обучения на более мелком и специфическом наборе данных.
- **Нейронная сеть** – математическая модель, состоящая из взаимосвязанных искусственных нейронов, организованных в слои, предназначенная для выполнения задач машинного обучения и обработки данных.
- **Токен** – минимальная единица текста, например, слово или символ. Применяется в обработке естественного языка для анализа и генерации текста.

# Перечень сокращений

- **API (Application Programming Interface)** - набор правил и инструментов для взаимодействия программного обеспечения. API предоставляет возможность различным приложениям обмениваться данными и функциональностью.
- **JSON (JavaScript Object Notation)** – лёгкий формат обмена данными. Формат легко читается человеком и парсируется компьютером.

# 1 Введение

Настоящий документ представляет собой руководство пользователя (далее руководство) системы Cotype.
Руководство описывает:
- общее определение системы;
- функции системы;
- эксплуатацию системы через API-запросы;
- взаимодействие с системой через веб-интерфейс.

## 1.1 Краткое описание возможностей

Cotype — это большая языковая модель для генерации и редактирования текстов, суммирования и анализа информации. Cotype включает в себя также веб-приложение для ее запуска и использования.

> В каждую модель, предоставляемую нашим клиентам и партнерам, встроен уникальный водяной знак. Это необходимо для установления факта утечки модели от клиента или партнера. Просим вас ответственно относиться к обеспечению безопасности хранения модели.

В следующей таблице приведены основные функции Cotype.

**Таблица 1**. Функции системы.

| Функция  | Описание | 
|-------|-----|
|Суммаризация текста| Позволяет пользователям получать краткое и информативное содержание предоставленных текстовых материалов. Эта функция особенно полезна для быстрой обработки больших объемов информации. Она  позволяет пользователям оперативно получать основные идеи и выводы без необходимости читать полный текст.  |
|Извлечение информации из текстовых данных| Пользователи могут получать конкретные данные из предоставленных текстов и выводить их в требуемом виде. Функция  включает в себя идентификацию и извлечение важных фактов, числовых значений, дат, имен собственных и других значимых элементов текста. Это существенно облегчает процесс анализа и использования информации для различных бизнес-потребностей.  |
|Творческая генерация| Обеспечивает создание оригинального текстового контента, такого как статьи, диалоги или рассказы, на основе заданных параметров и требований пользователя. Это позволяет компаниям быстро генерировать уникальный контент для маркетинговых материалов, блогов, презентаций и других целей, снижая зависимость от ручного труда и ускоряя процесс создания качественного текста.  |
|Классификация текстовых данных| Cotype осуществляет систематизацию и организацию объектов, явлений или понятий по определенным критериям. Это позволяет пользователям структурировать информацию, облегчая ее поиск и анализ. Классификация может быть основана на различных параметрах, таких как темы, категории, приоритеты или другие специфические требования, что обеспечивает гибкость и точность в управлении текстовыми данными.  |
|Ролевой отыгрыш| Позволяет системе отвечать от лица заданного персонажа или представителя определенной профессии, ведя диалог в стиле, соответствующем требуемой специальности. Это обеспечивает более естественное и релевантное взаимодействие с пользователями, что особенно важно для задач, связанных с клиентской поддержкой, обучением и другими  областями, требующими специфической коммуникации. |
| Ведение диалога с пользователем на естественном языке в режиме чата | Обеспечивает интерактивное и динамичное общение, позволяя пользователям получать ответы на вопросы, выполнять команды и получать поддержку в реальном времени. Высокий уровень понимания и генерации естественного языка обеспечивает комфортное и интуитивно понятное взаимодействие, что способствует улучшению пользовательского опыта и повышению удовлетворенности клиентов. |
| Генерация идей на заданную тематику | Дает возможность получать новые и креативные идеи, основанные на заданных темах или проблемах. Это особенно полезно для задач, связанных с инновациями, разработкой продуктов, маркетинговыми кампаниями и другими областями, где необходим свежий и оригинальный подход для решения поставленных задач. |

## 1.2 Уровень подготовки пользователей

Пользователи Системы должны:
- уметь взаимодействовать с системой через REST-API, осуществляя http-запросы через специализированные инструменты. Например, с помощью Postman и curl. 
- знать json-формат данных, используемый в API системы. 

## 1.3 Перечень эксплуатационной документации, с которой необходимо ознакомиться пользователю

Для работы в Системе, пользователь должен ознакомиться с настоящим руководством.

# 2 Назначение и условия применения

Система предназначена для обработки, генерации и анализа текстовых данных на основе запроса пользователя, сформулированного на естественном языке в свободной форме. 

Пользователь может взаимодействовать с LLM Cotype:
- через REST-API, отправляя http-запросы и получая ответы от LLM.
- через чат-интерфейс - веб-приложение, предназначенное для взаимодействия с LLM через веб-интерфейс.

> Для чат-интерфейса используется open source решение "NextChat".

# 3 Справочник по API

Справочник по API предоставляет базовую информацию для работы с продуктом и сообщениями в системе. 

## 3.1 POST/v1/chat/completions

Основным методом для взаимодействия с моделью является метод `POST/v1/chat/completions`. Он предназначен для отправки списка сообщений в формате чата. На основе полученных входных данных, модель сгенерирует ответ на запрос пользователя. Метод может использоваться как для ведения диалогов, состоящих из нескольких последовательных сообщений, так и для выполнения задач, требующих однократного обращения без продолжительного разговора.

**Тип запроса**: `POST`<br>
**Запрос**: [http://IP_Address/v1/chat/completions](http://IP_Address/v1/chat/completions)<br>
Где "IP_Address" необходимо заменить на IP-адрес вашей машины, если модель развёрнута внутри вашего контура. Если вы обращаетесь к демо-стенду внутри контура МТС ИИ, то адрес стенда будет передан вам отдельно.<br>
Например: [http://1.1.1.1:8000/v1/chat/completions](http://1.1.1.1:8000/v1/chat/completions)

**Таблица 2.** Параметры запроса `POST/v1/chat/completions`

| Имя | Тип данных | Значение | Описание  |
|-------|-----|----------|-------|
|messages (обязательный)| string | Текстовый ответ | Одно сообщение или список из нескольких сообщений в формате чата, на основе которых, модель должна сгенерировать ответ. |
|model (обязательный)| string | ID | ID модели, к которой вы обращаетесь. Вы можете узнать список доступных вам  моделей через запрос `GET/v1/models`. Подробнее в  разделе [3.2 - GET/v1/models](#32-getv1models). |
|temperature| float | Диапазон температуры — от 0 до 2. Рекомендованное значение: 0.5 | Более низкие значения температуры приводят к более последовательным результатам (например, 0.2), в то время как более высокие значения генерируют более разнообразные и творческие результаты (например, 1.0).  |
|top_p| integer | Диапазон — от 0 до 1. | Чем ниже значение атрибута, тем более популярные и часто встречающиеся токены модель будет использовать для формирования ответа. <br> Рекомендуем изменять или этот атрибут, или temperature, но не оба сразу. |
|max_tokens| integer | Натуральное число, больше 0. |Максимальное количество токенов, которые могут быть сгенерированы в ответ на запрос пользователя. Это позволяет регулировать длину ответа. |
|n| integer | Натуральное число больше 0. По умолчанию: 1.  |Количество ответов, которые модель сгенерирует. |
|stream| boolean | True/False. По умолчанию: false. |Если установить значение true, ответ модели будет возвращаться не целиком сразу, а итеративно, по мере его формирования моделью. |
|stream_options| object or null  |  <code> "stream": true,<br>&nbsp;&nbsp;"stream_options":{"include_usage": true} </code> |Параметры для потокового ответа. Устанавливайте только при установке `stream: true`.  |
|&nbsp;&nbsp; include_usage| boolean  | <code> "usage": {<br>&nbsp;&nbsp; "prompt_tokens": 61,<br>&nbsp;&nbsp; "total_tokens": 446,<br>&nbsp;&nbsp; "completion_tokens": 385<br> } </code> |Если установлен в true,  то будет добавлен новый информационный фрагмент перед последним фрагментом `[DONE]`. Поле `usage` в этом фрагменте показывает статистику использования токенов для всего запроса, а поле `choices` всегда будет пустым массивом. |
|frequency_penalty| integer | Натуральное число от -2 до 2. |Штраф за частоту — число между -2.0 и 2.0. Положительные значения штрафуют новые токены, на основе их текущей частоты в тексте, снижая вероятность того, что модель повторит одну и ту же строку. |
|presence_penalty| integer | Натуральное число от -2 до 2. |Положительные значения накладывают штраф на новые токены в зависимости от того, появляются ли они в тексте до сих пор, увеличивая вероятность того, что модель будет говорить о новых темах. |
|logit_bias | map | По умолчанию: null |Позволяет изменять  вероятность появления указанных токенов в генерации. Принимает объект JSON, который сопоставляет токены (указанные по их идентификатору токена в токенизаторе) со связанным значением отклонения от -100 до 100. `logit_bias` позволяет запретить модели использовать некоторые ID токенов. Чем ближе значение параметра к -100, тем больше вероятность, что токен будет заблокирован моделью.  Чем ближе значение параметра к + 100, тем больше вероятность что токен будет использован моделью. |
|logprobs | boolean or null | По умолчанию: false |Задает возвращать ли  логарифмические вероятности выходных токенов или нет. Если true, возвращает логарифмические вероятности  каждого выходного токена, возвращенного в содержимом сообщения. |
|top_logprobs | integer or null | |Целое число от 0 до 20, регулирующее количество наиболее вероятных токенов с их логарифмическими вероятностями, которые будут возвращены для каждого генерируемого токена. Если используется этот параметр, то `logprobs` должен быть установлен в значение true. |
|stop | string/array/null | |Список строк, после которых останавливается генерация. Эти строки не будут включены в ответ.  |
|parallel_tool_calls| boolean | По умолчанию: true |Определяет следует ли включать параллельный вызов функций/тулов. |
|tool_choice | string |  |Определяет как модель выбирает tools. Значения - **auto**, **none**, **required**, или задайте функцию. |
|tools | array|  |Список функций (тулов) с описаниями и аргументами, среди которых модель может выбрать необходимые тулы и извлечь значения аргументов из промпта для использования приложением. |
|&nbsp;&nbsp;function | object|  |Вызываемая функция. |
|&nbsp;&nbsp;&nbsp;&nbsp; description | string |  |Описание функции, включая инструкцию, когда и как ее вызвать. |
|&nbsp;&nbsp;&nbsp;&nbsp; name | string |  |Название функции. |
|&nbsp;&nbsp;&nbsp;&nbsp; parameters | object |  |Параметры функции в json. |
|&nbsp;&nbsp;&nbsp;&nbsp; strict | boolean or null  |  |Задает следует ли включать строгое соблюдение схемы при генерации вызова функции. Если установлено  значение **true**, модель будет точно соответствовать схеме, определенной в поле `parameters`. Если значение `strict` равно **true**,  поддерживается только подмножество схем JSON. |
|&nbsp;&nbsp;type  | string|  |tool тип, например функция. |
|user | string|  |Уникальный идентификатор, представляющий вашего конечного пользователя, который может помочь отслеживать и обнаруживать злоупотребления. |

### 3.1.1 Использование запроса POST/v1/chat/completions

1. Укажите **Bearer Token**.<br> При активной однотокеновой авторизации токен одинаковый у всех пользователей. При пользовательской авторизации выдаются индивидуальные токены.
2. Заполните тело запроса.<br> **Обязательные параметры**: `model` и `messages`. В `model` необходимо указать ID вашей модели. Получить ID модели можно с помощью запроса `GET /v1/models`, подробное описание смотрите в разделе [3.2 - GET /v1/models](#32-getv1models).<br> В `messages` необходимо указать "role" и "content". У атрибута "role" может быть одно из двух значений:
- "system" - обозначение для системного промпта, который задает роль модели, например, что модель должна отвечать как учитель или как политик. Необязательный атрибут;
- "user" -  обозначение пользовательского промпта, который содержит ваш вопрос или ваши инструкции для модели. Обязательный атрибут.<br> В параметр "content" запишите ваш системный или пользовательский промпт.

  ```
  {
    "model": "//ID вашей модели",
    "messages": [
      {
        "role": "system",
        "content": "Отвечай как экскурсовод"
      },
      {
        "role": "user",
        "content": "Расскажи мне о Москве в 1 предложении."
      }
    ]
  }
  ```
  Пример заполнения представлен ниже.

  ```
  {
    "model": "cotype_pro_16k_1.1",
    "messages": [
      {
        "role": "system",
        "content": "Отвечай как экскурсовод"
      },
      {
        "role": "user",
        "content": "Расскажи мне о Москве в 1 предложении."
      }
    ]
  }
  ```

  Пример curl-запроса для выполнения запроса из командной строки:

  ```
  curl  https://{url}/v1/chat/completions \
   -H "Content-Type: application/json" \
   -H "Authorization: Bearer Token" \
   -d '{
      "model": "cotype_pro_16k_1.1",
      "messages": [
        {
          "role": "system",
          "content": "Отвечай как экскурсовод"
        },
        {
          "role": "user",
          "content": " Расскажи мне о Москве в 1 предложении."
        }
      ]
    }'
   ```

### 3.1.2 Результат выполнения запроса POST/v1/chat/completions

Результат успешного запроса:

```
{
    "id": "cmpl-e263c7d6179a43e98b2ca9580b57e4f6",
    "object": "chat.completion",
    "created": 1724069159,
    "model": "cotype_pro_16k_1.1",
    "choices": [
        {
            "index": 0,
            "message": {
            	"role": "assistant",
            	"content": "Москва, столица России, - это тысячелетний город с богатой историей и архитектурой, где древнерусские храмы и кремли, таких как Красная площадь и Московский Кремль, соседствуют с современными небоскребами и шумными улицами, создавая уникальный контраст между прошлым и настоящим."
               "tool_calls": []
            },
            "logprobs": null,
            "finish_reason": "stop",
            "stop_reason": null
        }
    ],
    "usage": {
        "prompt_tokens": 89,
        "total_tokens": 193,
        "completion_tokens": 104
	  },
    "prompt_logprobs": null
}
```

**Таблица 3**. Параметры ответа на запрос POST/v1/chat/completions

| Имя | Тип данных | Пример значение |Описание  |
|-------|---------|----------|-------|
|id| string | cmpl-e263c7d6179a43e98b2ca9580b57e4f6 | Идентификатор запроса. |
|object| string | chat.completion | Тип объект. |
|created| integer | 1724069159 | Временная метка UNIX (в секундах), отмечающая дату и время, когда был создан запрос. |
|model| string | cotype_pro_16k_1.1 | ID модели, к которой вы обращались и которая ответила на ваш запрос. |
|choices| array |  | Список вариантов завершения чата. |
|&nbsp;&nbsp; index| integer | 0 | Индекс выбора в списке вариантов. |
|&nbsp;&nbsp; message| object |  | Сгенерированное моделью сообщение. |
|&nbsp;&nbsp;&nbsp;&nbsp; role | string | assistant | Роль автора сообщения. |
|&nbsp;&nbsp;&nbsp;&nbsp;  content  | string or null | Москва, столица России, - это тысячелетний город с богатой историей и архитектурой, где древнерусские храмы и кремли, таких как Красная площадь и Московский Кремль, соседствуют с современными небоскребами и шумными улицами, создавая уникальный контраст между прошлым и настоящим.  | Содержание сообщения. |
|&nbsp;&nbsp;&nbsp;&nbsp;  tool_calls  | array | []  | Вызовы тулов (функций), генерируемые моделью.  |
|&nbsp;&nbsp;  logprobs  | object or null | null  | Логирование информации о вероятности выбора.  |
|&nbsp;&nbsp;  finish_reason  | string or null | stop  | Причина, по которой модель прекратила  генерировать ответы. <br> `stop` -  если модель достигла естественной точки остановки или предоставленной последовательности остановки. <br> `length` - если было достигнуто максимальное количество токенов, указанное в запросе. <br> `content_filter` -  если контент был пропущен из-за флага из фильтров контента. <br> `tool_calls` -  если модель вызвала `tool`, или ​​если модель вызвала функцию. |
|&nbsp;&nbsp;  stop_reason  | string or null |  | Аналогична `finish_reason`. |
|usage   | object or null |  <code>"prompt_tokens": 89,<br> "total_tokens": 193, <br>"completion_tokens": 104 </code>| Статистика по токенам в вашем запросе. Это поле присутствует в ответе на запрос, если при установке `stream_options: {"include_usage": true}` в запросе. <br> Если поле присутствует, поле содержит нулевое значение, за исключением последнего фрагмента, который содержит статистику использования токена для всего запроса. |
|&nbsp;&nbsp;prompt_tokens | integer |  39| Количество токенов в запросе. |
|&nbsp;&nbsp; completion_tokens | integer |  91| Количество сгенерированных моделью токенов. |
|&nbsp;&nbsp; total_tokens | integer |  130| Cумма токенов в запросе и сгенерированных токенов. |
|prompt_logprobs | object or null |  null| Логирование информации о промпте. |

Для продолжения общения с моделью в рамках текущего диалога необходимо дополнить ваш предыдущий запрос ответом модели, а также добавить новое сообщение от лица пользователя. Атрибут `role` показывает, кто является автором сообщения в диалоге. `Assistant` — ответ модели, `user` — сообщение пользователя.

Пример тела запроса:

```
{
  "model": "//имя вашей модели",
  "messages": [
    {"role": "system","content": "Отвечай как экскурсовод"},
    {"role": "user", "content": "Расскажи мне о Москве в 1 предложении."},
    {"role": "assistant", "content": "Москва - это столица России, древний и современный город, объединяющий богатую историю и культуру, архитектурные шедевры, таких как Кремль и храм Василия Блаженного, с динамичной городской жизнью, современными технологиями и инфраструктурой, а также уникальной атмосферой, которая сочетает в себе традиции и инновации." },
    {"role": "user", "content": "Расскажи мне более подробно о Кремле."} 
  ]
}
```

Модель сформирует ответ на ваш запрос с учётом контекста диалога. В данном примере расскажет именно о московском Кремле, а не о казанском или новгородском.

### 3.1.3 Пример выполнения запроса POST/v1/chat/completions с функцией tool calling

```
import requests
import json

def get_weather(location: str, unit: str):
    return f"Получить погоду для {location} в {unit}..."

tool_functions = {"get_weather": get_weather}

tools = [{
    "type": "function",
    "function": {
        "name": "get_weather",
        "description": "Определить текущую погоду в городе",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "Город или поселок, например, 'Москва'"},
                "unit": {"type": "string", "enum": ["цельсии", "фаренгейты"]}
            },
            "required": ["location", "unit"]
        }
    }
}]

# URL API
url = "http://localhost:8000/v1/chat/completions"

# Заголовки запроса
headers = {
    "Content-Type": "application/json",
    "Authorization": "Bearer dummy"
}

# Тело запроса
data = {
    "model": "your-model-name",  # Замените на имя вашей модели
    "messages": [{"role": "user", "content": "Как тепло сейчас в Санкт-Петербурге?"}],
    "tools": tools,
    "tool_choice": "auto"
}

# Отправка POST-запроса
response = requests.post(url, headers=headers, json=data)

# Проверка статуса ответа
if response.status_code == 200:
    response_data = response.json()
    tool_call = response_data['choices'][0]['message']['tool_calls'][0]['function']
    
    print(f"Function called: {tool_call['name']}")
    print(f"Arguments: {tool_call['arguments']}")
    print(f"Result: {get_weather(**json.loads(tool_call['arguments']))}")
else:
    print(f"Error: {response.status_code}")
    print(response.text)
```

Ответ от модели:

```
{
 'id': 'chatcmpl-080f745514ee4e27bffca4e567ba3c4a',
 'object': 'chat.completion',
 'created': 1741944362,
 'model': 'cotype',
 'choices': [{'index': 0,
   'message': {'role': 'assistant',
    'content': None,
    'tool_calls': [{'id': 'chatcmpl-tool-8e49f569510b4cf6afd5524aee967647',
      'type': 'function',
      'function': {'name': 'get_weather',
       'arguments': '{"location": "Санкт-Петербург", "unit": "цельсии"}'}}]},
   'logprobs': None,
   'finish_reason': 'tool_calls',
   'stop_reason': None}],
 'usage': {'prompt_tokens': 261,
  'total_tokens': 294,
  'completion_tokens': 33,
  'prompt_tokens_details': None},
 'prompt_logprobs': None
}
```

## 3.2 GET/v1/models

Метод для получения списка доступных вам моделей. <br> 
**Тип запроса**: `GET` <br>
Запрос: 

```
 http://IP_Address/v1/models
```
где `IP_Address` необходимо заменить на IP-адрес вашей машины или адрес демо-стенда. Например:

```
 http://1.1.1.1:8000/v1/models
```
Пример curl-запроса для выполнения запроса из командной строки:

```
 curl -X 'GET' \  'https://{url}/v1/models' \
  -H 'accept: application/json'
  -H 'Authorization: Bearer Token' \
```

Пример ответа на запрос `GET/v1/models`:

```
{
    "object": "list",
    "data": [
        {
            "id": "cotype_pro_16k_1.1",
            "object": "model",
            "created": 1724087433,
            "owned_by": "vLLM",
            "root": "cotype_pro_16k_1.1",
            "parent": null,
            "permission": [
                {
                    "id": "modelperm-1b7dd7dea5244bdb888971bf2b89f6ff",
                    "object": "model_permission",
                    "created": 1724087433,
                    "allow_create_engine": false,
                    "allow_sampling": true,
                    "allow_logprobs": true,
                    "allow_search_indices": false,
                    "allow_view": true,
                    "allow_fine_tuning": false,
                    "organization": "*",
                    "group": null,
                    "is_blocking": false
                }
            ]
        }
    ]
}
```

В результате выполнения запроса, вы получите список доступных вам моделей. Для обращения к модели скопируйте "id" - уникальный идентификатор модели в теле ответа на запрос. Затем используйте скопированный ID в запросе `POST/v1/chat/completions` в параметре `model`.

## 3.3 GET/health

**Назначение**: Запрос проверяет работоспособность модели. Если модель работоспособна, будет получен ответ `[200 OK]`. В случае если сервис недоступен по какой-либо причине, ответ не будет получен. <br>
**Тип запроса**: `GET` <br>
**Запрос**: 

```
http://IP_Address/health
```

Где `IP_Address` необходимо заменить на IP-адрес вашей машины или адрес демо-стенда. Например:

```
http://1.1.1.1:8000/health
```

Пример curl-запроса для выполнения запроса из командной строки:

```
 curl -X 'GET' \
  'https://{url}/health' \
  -H 'accept: application/json'
```


## 3.4 POST/v1/completions 

**Назначение**: Система дополняет и продолжает промпт пользователя. Не подходит для общения в чатовом режиме. <br>
**Тип запроса**:  `POST` <br>
**Запрос**: 

```
https://{IP-адрес}/v1/completions
```

Где `IP_Address` необходимо заменить на IP-адрес вашей машины или адрес демо-стенда. Например:

```
 http://1.1.1.1:8000/v1/completions
```

**Таблица 4**. Параметры запроса POST/v1/completions

| Имя | Тип данных | Значение |Описание  |
|-------|---------|----------|-------|
|prompt (обязательный) | string or array |  | Запрос(ы) для генерации дополнений, закодированные как строка, массив строк, массив токенов или массив массивов токенов. Обратите внимание, что <code><&#166;endoftext&#166;></code> — это разделитель документов, который модель видит во время обучения, поэтому, если запрос не указан, модель будет генерировать так, как будто с начала нового документа. |
|model (обязательный)  | string | ID | ID модели, к которой вы обращаетесь. |
|temperature | number or null  |Диапазон температуры — от 0 до 2. По умолчанию: 1. | Задает  температуру выборки, от 0 до 2. Более высокие значения, такие как 0.8, сделают вывод более случайным, а более низкие значения, такие как 0.2, сделают его более сфокусированным и детерминированным. Обычно рекомендуется изменять это значение или `top_p` параметр, но не оба. |
|top_p | number or null |Диапазон — от 0 до 1. По умолчанию: 1. | Чем ниже значение атрибута, тем более популярные и часто встречающиеся токены модель будет использовать для формирования ответа. Рекомендуем изменять или этот атрибут, или `temperature`, но не оба сразу. |
|max_tokens| integer |Натуральное число, больше 0. По умолчанию: 16. | Максимальное количество токенов, которые могут быть сгенерированы в ответ на запрос пользователя. Это позволяет регулировать длину ответа. |
|n| integer or null |Натуральное число больше 0. По умолчанию: 1. | Количество ответов, которые модель сгенерирует. |
|stream| boolean or null |True/False. По умолчанию: false. | Если установить значение **true**, ответ модели будет возвращаться не целиком сразу, а итеративно, по мере его формирования моделью. |
|stream_options | object or null  |<code>"stream": true,<br> "stream_options":{"include_usage": true} </code>  |Параметры для потокового ответа. Устанавливайте только при установке `stream: true`. |
|&nbsp;&nbsp;include_usage | boolean | |Если установлено, дополнительный фрагмент будет передан перед сообщением data: `[DONE]`. Поле `usage` в этом фрагменте показывает статистику использования токена для всего запроса, а поле `choices` всегда будет пустым массивом. Все остальные фрагменты также будут включать поле `usage`, но с "null" значением. |
|frequency_penalty | integer |Натуральное число от -2 до 2. |Штраф за частоту — число между -2.0 и 2.0. Положительные значения штрафуют новые токены, на основе их текущей частоты в тексте, снижая вероятность того, что модель повторит одну и ту же строку. |
|presence_penalty | integer |Натуральное число от -2 до 2. По умолчанию: 0. |Положительные значения накладывают штраф на новые токены в зависимости от того, появляются ли они в тексте до сих пор, увеличивая вероятность того, что модель будет говорить о новых темах. |
|logit_bias  | map |По умолчанию: null. |Изменяет вероятность появления указанных токенов в завершении. Принимает объект JSON, который сопоставляет токены (указанные по их идентификатору токена в токенизаторе) со связанным значением смещения от -100 до 100. |
|best_of  | integer or null  |По умолчанию: 1. |Генерирует `best_of` завершений на стороне сервера и возвращает «лучшее» (то, которое имеет наибольшую вероятность логарифма на токен). Результаты не могут быть переданы потоком. При использовании с `n`, `best_of` управляет количеством кандидатов на завершение, а `n` указывает, сколько возвращать. `best_of` должно быть больше `n`. |
|echo  | boolean or null |По умолчанию: false. |Повторяет промпт в дополнение к завершению. |
|logprobs | integer or null  |По умолчанию: null. |Включает логарифмические вероятности на `logprobs` наиболее вероятных выходных токенах, а также выбранные токены. Например, если `logprobs` равен 5, API вернет список из 5 наиболее вероятных токенов. |
|seed | integer or null  | |Если параметр задан, система приложит все усилия для детерминированной выборки, чтобы повторные запросы с тем же начальным значением и параметрами возвращали тот же результат. |
|stop| string/array/null | |До 4 последовательностей,  при которых API прекратит генерацию дальнейших токенов. |
|suffix| string or null  | По умолчанию: null. |Суффикс, который следует после завершения вставленного текста. |
|user| string | |Уникальный идентификатор, представляющий вашего конечного пользователя, который может помочь отслеживать и обнаруживать злоупотребления. |

### 3.4.1 Использование запроса POST/v1/completions

1. Укажите **Bearer Token**. 
 
   При активной однотокеновой авторизации токен одинаковый у всех пользователей. При пользовательской авторизации выдаются индивидуальные токены.
2. Заполните тело запроса. <br>
**Обязательные параметры**: `model` и `prompt`. В `model` необходимо указать ID вашей модели. Получить ID модели можно с помощью запроса `GET/v1/models`, подробное описание смотрите в разделе [3.2 - GET/v1/models](#32-getv1models). <br>
Пример curl-запроса для выполнения запроса из командной строки:

   ```
    curl -X 'POST' \
     'https://{IP-адрес}/v1/completions' \
     -H 'accept: application/json'
    curl  https://{IP-адрес}/v1/completions \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer Token" \
    -d '{"model": "cotype_pro_16k_1.1", "prompt": "Тестовый запрос"}'
   ```
   
 ### 3.4.2 Результат выполнения запроса POST/v1/completions

Результат успешного запроса:

```
{
    "id": "cmpl-b1e39b7c457f46e2af97b47e921cf64c",
    "object": "text_completion",
    "created": 1740518640,
    "model": "cotype_pro_16k_1.1",
    "choices": [
        {
            "index": 0,
            "text": "! Как дела?\r",
            "logprobs": null,
            "finish_reason": "stop",
            "stop_reason": "\n",
            "prompt_logprobs": null
        }
    ],
    "usage": {
        "prompt_tokens": 116,
        "total_tokens": 121,
        "completion_tokens": 5
    }
}
```

**Таблица 5**. Параметры ответа на запрос POST/v1/completions

| Имя | Тип данных | Пример значения |Описание  |
|-------|---------|----------|-------|
|id | string | cmpl-b1e39b7c457f46e2af97b47e921cf64c  | Идентификатор запроса. |
|object| string | text.completion  | Тип объект. |
|created| integer  | 1740518640  | Временная метка UNIX (в секундах), отмечающая дату и время, когда был создан запрос. |
|model| string  | cotype_pro_16k_1.1  | ID модели, к которой вы обращались и, которая ответила на ваш запрос. |
|choices| array  |  |Список вариантов завершения чата.  |
|&nbsp;&nbsp; index| integer  | 0 |Индекс выбора в списке вариантов.  |
|&nbsp;&nbsp; text| string  | |Сгенерированное моделью сообщение.  |
|&nbsp;&nbsp;  logprobs | object or null |null |Логирование информации о вероятности выбора.  |
|&nbsp;&nbsp;  finish_reason | string  |stop |Причина, по которой модель прекратила генерировать ответы.<br> `stop` -  если модель достигла естественной точки остановки или предоставленной последовательности остановки. <br> `length` - если было достигнуто максимальное количество токенов, указанное в запросе. <br>`content_filter` -  если контент был пропущен из-за флага из фильтров контента. <br> `tool_calls` -  если модель вызвала инструмент, или ​​если модель вызвала функцию.  |
|&nbsp;&nbsp;  stop_reason | string or null  | |Аналогична `finish_reason`. |
|&nbsp;&nbsp;  prompt_logprobs | object or null  | null|Логирование информации о промпте.|
|usage | object or null  | <code>"prompt_tokens": 116,<br>"total_tokens": 121,<br>"completion_tokens": 5</code>|Статистика запроса на завершение.|
|&nbsp;&nbsp;  prompt_tokens |integer  | 39|Количество токенов в промпте. |
|&nbsp;&nbsp;  completion_tokens |integer  | 91|Количество сгенерированных моделью токенов. |
|&nbsp;&nbsp;  total_tokens |integer  | 130|Cумма токенов в запросе и сгенерированных токенов.  |

## 3.5 POST/v1/embeddings

**Назначение**: Запрос трансформирует отправленный текст в эмбеддинги. Предназначен для представления текста в векторном виде. <br>
**Тип запроса**:  `POST` <br>
**Запрос**: 

```
https://IP_Address/v1/embeddings
```
Где `IP_Address` необходимо заменить на IP-адрес вашей машины или адрес демо-стенда. Например:

```
 http://1.1.1.1:8000/v1/embeddings
```
Пример curl-запроса для выполнения запроса из командной строки:

```
 curl -X 'POST' \
  'https://{IP-адрес}/v1/embeddings' \
  -H 'accept: application/json'
 curl  https://{IP-адрес}/v1/embeddings \
 -H "Content-Type: application/json" \
 -H "Authorization: Bearer Token" \
 -d '{"text":  ["Тестовый запрос"]}'
```
Пример ответа на запрос `POST/v1/embeddings`:

```
{
    "object": "list",
    "data": [
        {
            "object": "embedding",
            "index": 0,
            "embedding": [0.8983038067817688,
                0.09742391109466553,
                -0.9155033230781555,
                … 
             ]
         }
     ],
    "model": "mtsai-chat-embeddings",
    "usage": {
        "prompt_tokens": 6,
        "total_tokens": 6
    }
}
```

## 3.6 POST/token

**Назначение**: Получение токена доступа для работы с API. <br>
**Тип запроса**:  `POST` <br>
**Запрос**: 

```
 https://IP_Address/token
```

Где `IP_Address` необходимо заменить на IP-адрес вашей машины или адрес демо-стенда. Например:

```
 http://1.1.1.1:8000/token
```
Пример curl-запроса для выполнения запроса из командной строки:

```
 curl -X 'POST' \
  'https://{url}/token' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/x-www-form-urlencoded' \
  -d 'username=your.email@example.ru&password=your_password'
```

Пример ответа на запрос:

```
{
  "access_token": "string",
  "token_type": "string"
}
```

# 4 Работа с чат-интерфейсом 

## 4.1 Аутентификация
